{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52384081-c18d-4422-963a-aef3c0889175",
   "metadata": {},
   "source": [
    "# Simulated transactions analysis - big data project\n",
    "\n",
    "## Main jobs:\n",
    "<ol>\n",
    "  <li>obtain metrics by (expense_type, year) and (spending_class, year)</li>\n",
    "  <li>calculate churn rate, cvv (customer loyalty), and other analytics</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3204941-cbd5-4084-8a9b-6d95a8f8058d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark\r\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ca304e9-da18-4756-bb56-4d892f03a0eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SaveMode\r\n",
       "import org.apache.spark.HashPartitioner\r\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SaveMode\n",
    "import org.apache.spark.HashPartitioner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43f61fa6-19c9-4f94-9e05-e2341784f543",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import java.time.LocalDate\r\n",
       "import java.time.format.DateTimeFormatter\r\n",
       "import java.time.temporal.ChronoUnit\r\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import java.time.LocalDate\n",
    "import java.time.format.DateTimeFormatter\n",
    "import java.time.temporal.ChronoUnit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f16ef6-9b12-4330-8e43-bc3eb74e4dee",
   "metadata": {},
   "source": [
    "## Case classes\n",
    "\n",
    "After making imports, Transaction and CustomerSpending case classes are istanciated, in order to mantain a more readable code. It can be mapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4687f751-ab44-452b-a897-aa753461c030",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class Transaction\r\n",
       "defined class CustomerSpending\r\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// case class definition for data clarity\n",
    "\n",
    "case class Transaction(\n",
    "  custId: String,\n",
    "  startDate: String,\n",
    "  endDate: String,\n",
    "  transId: String,\n",
    "  date: LocalDate,\n",
    "  year: Int,\n",
    "  month: Int,\n",
    "  day: Int,\n",
    "  expType: String,\n",
    "  amount: Double\n",
    ")\n",
    "\n",
    "case class CustomerSpending(totalSpend: Double, spendingClass: String)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f1d2d04-f451-49d3-8b3a-4d47550030cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd: org.apache.spark.rdd.RDD[String] = sample_data_big_data.csv MapPartitionsRDD[1] at textFile at <console>:42\r\n",
       "header: String = CUST_ID,START_DATE,END_DATE,TRANS_ID,DATE,YEAR,MONTH,DAY,EXP_TYPE,AMOUNT\r\n",
       "dataRdd: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[2] at filter at <console>:44\r\n",
       "transactionsRdd: org.apache.spark.rdd.RDD[Transaction] = MapPartitionsRDD[3] at map at <console>:47\r\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// rdd mapping with created custom case classes \n",
    "\n",
    "val rdd = sc.textFile(\"sample_data_big_data.csv\")\n",
    "val header = rdd.first()\n",
    "val dataRdd = rdd.filter(row => row != header)\n",
    "\n",
    "\n",
    "val transactionsRdd = dataRdd.map { line =>\n",
    "    val fields = line.split(\",\")\n",
    "    val formatter = DateTimeFormatter.ofPattern(\"yyyy-MM-dd\")\n",
    "    Transaction(\n",
    "        custId   = fields(0),\n",
    "        startDate = fields(1),\n",
    "        endDate   = fields(2),\n",
    "        transId   = fields(3),\n",
    "        date      = LocalDate.parse(fields(4), formatter),\n",
    "        year      = fields(5).toInt,\n",
    "        month     = fields(6).toInt,\n",
    "        day       = fields(7).toInt,\n",
    "        expType   = fields(8),\n",
    "        amount    = fields(9).toDouble\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0b3d2db1-4e57-4681-8816-8fec69590f83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "customerSpendingsRdd: org.apache.spark.rdd.RDD[(String, Double)] = ShuffledRDD[5] at reduceByKey at <console>:43\r\n",
       "classifiedSpendingsRdd: org.apache.spark.rdd.RDD[(String, CustomerSpending)] = MapPartitionsRDD[6] at map at <console>:46\r\n",
       "transactionsJoinedRdd: org.apache.spark.rdd.RDD[(String, String, String, Int, String, Double)] = MapPartitionsRDD[11] at map at <console>:57\r\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// total spendings per customer\n",
    "val customerSpendingsRdd = transactionsRdd\n",
    "  .map(t => (t.custId, t.amount))\n",
    "  .reduceByKey(_ + _)\n",
    "\n",
    "// customer spendings classification\n",
    "val classifiedSpendingsRdd = customerSpendingsRdd.map { case (custId, totalSpend) =>\n",
    "  val spendingClass = if (totalSpend < 1000.0) \"Poor\"\n",
    "                      else if (totalSpend < 10000.0) \"Middle\"\n",
    "                      else \"Rich\"\n",
    "  (custId, CustomerSpending(totalSpend, spendingClass))\n",
    "}\n",
    "\n",
    "// inner join transactions with classified spendings\n",
    "val transactionsJoinedRdd = transactionsRdd\n",
    "  .map(t => (t.custId, t))\n",
    "  .join(classifiedSpendingsRdd)  // (custId, (Transaction, CustomerSpending))\n",
    "  .map { case (custId, (transaction, custSpending)) =>\n",
    "    (transaction.custId, transaction.transId, custSpending.spendingClass, transaction.year, transaction.expType, transaction.amount)\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3d0c78f8-2a97-47d7-ac7b-7052596a908b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "aggregatedByCategoryYear: org.apache.spark.rdd.RDD[((String, Int), (Double, scala.collection.immutable.Set[String]))] = ShuffledRDD[13] at reduceByKey at <console>:45\r\n",
       "metricsByCategoryYear: org.apache.spark.rdd.RDD[((String, Int), (Double, Int, Double))] = MapPartitionsRDD[14] at map at <console>:49\r\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// criteria: (spending class, year)\n",
    "\n",
    "// Mapping: ((spendingClass, year), (amount, Set(customerId)))\n",
    "val aggregatedByCategoryYear = transactionsJoinedRdd\n",
    "  .map { case (custId, transId, spendingClass, year, expType, amount) =>\n",
    "    ((spendingClass, year), (amount, Set(custId)))\n",
    "  }\n",
    "  .reduceByKey { case ((amount1, custSet1), (amount2, custSet2)) =>\n",
    "    (amount1 + amount2, custSet1 ++ custSet2)\n",
    "  }\n",
    "\n",
    "val metricsByCategoryYear = aggregatedByCategoryYear.map { case ((spendingClass, year), (totalAmount, custSet)) =>\n",
    "  val customerCount = custSet.size\n",
    "  val avgSpend = totalAmount / customerCount\n",
    "  ((spendingClass, year), (totalAmount, customerCount, avgSpend))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "777ebc62-87b8-414a-a377-24bd74ff842d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "aggregateByExpenseType: org.apache.spark.rdd.RDD[((String, Int), (Double, scala.collection.immutable.Set[String]))] = ShuffledRDD[16] at reduceByKey at <console>:44\r\n",
       "metricsByExpenseType: org.apache.spark.rdd.RDD[((String, Int), (Double, Int, Double))] = MapPartitionsRDD[17] at map at <console>:48\r\n",
       "res1: Array[((String, Int), (Double, Int, Double))] = Array(((Housing,2018),(74269.59,40,1856.73975)), ((Entertainment,2016),(83579.70999999999,3162,26.432545857052496)), ((Bills and Utilities,2016),(34274.36,178,192.55258426966293)), ((Motor/Travel,2013),(32845.76999999998,443,74.1439503386004)), ((Education,2017),(19444.319999999996,81,240.05333333333328)), ((Tax,2017),(30339.549999999996,59,514.2296610169491)), ((Entertainment,2015),(72523.83000000002,2817,25.74505857294995)), ((Bills and ...\r\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// criteria (expense type (exp), amount)\n",
    "\n",
    "val aggregateByExpenseType = transactionsJoinedRdd\n",
    "    .map { case (custId, transId, spendingClass, year, expType, amount) => \n",
    "        ((expType, year), (amount, Set(custId)))\n",
    "    }\n",
    "    .reduceByKey { case ((amount1, custSet1), (amount2, custSet2)) =>\n",
    "        (amount1 + amount2, custSet1 ++ custSet2)\n",
    "    }\n",
    "\n",
    "val metricsByExpenseType = aggregateByExpenseType.map { case ((expType, year), (totalAmount, custSet)) =>\n",
    "  val customerCount = custSet.size\n",
    "  val avgSpend = totalAmount / customerCount\n",
    "  ((expType, year), (totalAmount, customerCount, avgSpend))\n",
    "}\n",
    "\n",
    "metricsByExpenseType.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc24467-72cf-4644-9219-b1771c6f9382",
   "metadata": {},
   "source": [
    "# CVC\n",
    "\n",
    "This section shows how you can manually extract features and informations from a dataset in order to create the CVC formula. This formula helps us to determine the customer value, and it helps deciding if we want to keep him."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "32dda1e0-b94a-4a84-ad1b-e8b21f1e319f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "formatter: java.time.format.DateTimeFormatter = Value(YearOfEra,4,19,EXCEEDS_PAD)'-'Value(MonthOfYear,2)'-'Value(DayOfMonth,2)\r\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val formatter = DateTimeFormatter.ofPattern(\"yyyy-MM-dd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b32547e8-7d99-4396-932e-43b787d22583",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "first_and_last_transaction_rdd: org.apache.spark.rdd.RDD[(String, (java.time.LocalDate, java.time.LocalDate))] = ShuffledRDD[33] at reduceByKey at <console>:42\r\n"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val first_and_last_transaction_rdd = transactionsRdd\n",
    "    .map(t => (t.custId, (t.date, t.date)))\n",
    "    .reduceByKey { case ((first1, last1), (first2, last2)) => \n",
    "        (if (first1.isBefore(first2)) first1 else first2, if (last1.isAfter(last2)) last1 else last2)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8a6c0469-d97d-4f7f-9fd9-9164d59dff52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "frequencyRdd: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[35] at reduceByKey at <console>:42\r\n"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val frequencyRdd = transactionsRdd\n",
    "    .map(t => (t.custId, 1))\n",
    "    .reduceByKey(_ + _)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b838b424-0c82-415a-a374-435cc581516e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import java.time.temporal.ChronoUnit\r\n",
       "clvFeaturesRdd: org.apache.spark.rdd.RDD[(String, java.time.LocalDate, java.time.LocalDate, Int, Double, Double, Double, Double)] = MapPartitionsRDD[28] at map at <console>:45\r\n",
       "res4: Array[(String, java.time.LocalDate, java.time.LocalDate, Int, Double, Double, Double, Double)] = Array((CI0BAX5OMV,2019-07-25,2019-07-25,1,4.13,4.13,0.0027397260273972603,0.011315068493150685))\r\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import java.time.temporal.ChronoUnit;\n",
    "\n",
    "val clvFeaturesRdd = first_and_last_transaction_rdd\n",
    "    .join(frequencyRdd)\n",
    "    .join(customerSpendingsRdd)\n",
    "    .map { case (custId, (((firstDate, lastDate), freq), totalSpend)) =>  \n",
    "        val lifetimeDays = ChronoUnit.DAYS.between(firstDate, lastDate) + 1\n",
    "        val avgSpendingPerTxn = totalSpend / freq\n",
    "        val estimatedLifetimeYears = lifetimeDays / 365.0\n",
    "        val clv = (avgSpendingPerTxn * freq) * estimatedLifetimeYears  // Formula CLV base\n",
    "        \n",
    "        (custId, firstDate, lastDate, freq, totalSpend, avgSpendingPerTxn, estimatedLifetimeYears, clv)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "58d12c2a-1f88-43cc-a721-4985a019afd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "customerSegmentsRdd: org.apache.spark.rdd.RDD[(String, String, Double)] = MapPartitionsRDD[29] at map at <console>:40\r\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val customerSegmentsRdd = clvFeaturesRdd.map { case (custId, firstDate, lastDate, freq, totalSpend, avgSpendingPerTxn, estimatedLifetimeYears, clv) =>\n",
    "  val segment = if (clv < 1000) \"Low Value\"\n",
    "                else if (clv < 5000) \"Medium Value\"\n",
    "                else \"High Value\"\n",
    "  (custId, segment, clv)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9dacb414-8ba9-4f8a-b506-ad1abf2a0d9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loyaltyCategoryRdd: org.apache.spark.rdd.RDD[(String, String, Double)] = MapPartitionsRDD[30] at map at <console>:40\r\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val loyaltyCategoryRdd = clvFeaturesRdd.map { case (custId, _, _, _, _, _, estimatedLifetimeYears, _) =>\n",
    "  val loyaltyCategory = if (estimatedLifetimeYears < 1) \"New Customer\"\n",
    "                        else if (estimatedLifetimeYears < 3) \"Regular Customer\"\n",
    "                        else \"Loyal Customer\"\n",
    "  (custId, loyaltyCategory, estimatedLifetimeYears)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f1c35d68-8944-4cab-818f-b0254d8a47cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "churnRiskRdd: org.apache.spark.rdd.RDD[(String, String, Long, Double)] = MapPartitionsRDD[31] at map at <console>:40\r\n"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val churnRiskRdd = clvFeaturesRdd.map { case (custId, _, lastDate, _, _, _, _, clv) =>\n",
    "  val today = LocalDate.now()\n",
    "  val daysSinceLastPurchase = ChronoUnit.DAYS.between(lastDate, today)\n",
    "\n",
    "  val churnRisk = if (clv < 1000 && daysSinceLastPurchase > 180) \"High Risk\"\n",
    "                  else if (daysSinceLastPurchase > 90) \"Medium Risk\"\n",
    "                  else \"Low Risk\"\n",
    "\n",
    "  (custId, churnRisk, daysSinceLastPurchase, clv)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64cccbae-bde3-47c8-8cf3-1275c93e558c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
