{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1cd3dbf-87f5-4f51-ae6f-ac332f5a74cb",
   "metadata": {},
   "source": [
    "# Simulated transactions analysis - Main Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31770849-7892-4d25-b25c-4f8d61f29ec8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark\r\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b788e6b-aca0-49d3-b171-94358d8e4461",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SaveMode\r\n",
       "import org.apache.spark.HashPartitioner\r\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SaveMode\n",
    "import org.apache.spark.HashPartitioner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e3ba76b-62dc-4045-87b7-51c83c00f757",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import java.time.LocalDate\r\n",
       "import java.time.format.DateTimeFormatter\r\n",
       "import java.time.temporal.ChronoUnit\r\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import java.time.LocalDate\n",
    "import java.time.format.DateTimeFormatter\n",
    "import java.time.temporal.ChronoUnit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9c9f04-0a9e-4482-bfe5-3539354c57c0",
   "metadata": {},
   "source": [
    "## Case classes \n",
    "\n",
    "After making imports, Transaction and CustomerSpending case classes are instanciated, in order to maintain a more readable code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6bcfb2df-ed0d-4e7c-aafe-ed0c7ee6cdf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class Transaction\r\n",
       "defined class CustomerSpending\r\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// case class definition for data clarity\n",
    "\n",
    "case class Transaction(\n",
    "  custId: String,\n",
    "  startDate: String,\n",
    "  endDate: String,\n",
    "  transId: String,\n",
    "  date: LocalDate,\n",
    "  year: Int,\n",
    "  month: Int,\n",
    "  day: Int,\n",
    "  expType: String,\n",
    "  amount: Double\n",
    ")\n",
    "\n",
    "case class CustomerSpending(totalSpend: Double, spendingClass: String)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ec320989-8db7-4822-a091-18c7414e2797",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd: org.apache.spark.rdd.RDD[String] = sample_data_big_data.csv MapPartitionsRDD[31] at textFile at <console>:39\r\n",
       "header: String = CUST_ID,START_DATE,END_DATE,TRANS_ID,DATE,YEAR,MONTH,DAY,EXP_TYPE,AMOUNT\r\n",
       "dataRdd: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[32] at filter at <console>:41\r\n",
       "transactionsRdd: org.apache.spark.rdd.RDD[Transaction] = MapPartitionsRDD[33] at map at <console>:44\r\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// rdd mapping with created custom case classes \n",
    "\n",
    "val rdd = sc.textFile(\"sample_data_big_data.csv\")\n",
    "val header = rdd.first()\n",
    "val dataRdd = rdd.filter(row => row != header)\n",
    "\n",
    "\n",
    "val transactionsRdd = dataRdd.map { line =>\n",
    "    val fields = line.split(\",\")\n",
    "    val formatter = DateTimeFormatter.ofPattern(\"yyyy-MM-dd\")\n",
    "    Transaction(\n",
    "        custId   = fields(0),\n",
    "        startDate = fields(1),\n",
    "        endDate   = fields(2),\n",
    "        transId   = fields(3),\n",
    "        date      = LocalDate.parse(fields(4), formatter),\n",
    "        year      = fields(5).toInt,\n",
    "        month     = fields(6).toInt,\n",
    "        day       = fields(7).toInt,\n",
    "        expType   = fields(8),\n",
    "        amount    = fields(9).toDouble\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee095fc8-d026-4734-a0b6-0c40cf49f10f",
   "metadata": {},
   "source": [
    "## Classified spendings\n",
    "\n",
    "First thing first, it's needed to understand every single customer spends, in order to obtain the spending class.\n",
    "After that, a join is made between transactions and classified spendings, in order to obtain a more complete dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f0da1149-5b89-456c-991e-14e754497c9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "customerSpendingsRdd: org.apache.spark.rdd.RDD[(String, Double)] = ShuffledRDD[51] at reduceByKey at <console>:39\r\n",
       "classifiedSpendingsRdd: org.apache.spark.rdd.RDD[(String, CustomerSpending)] = MapPartitionsRDD[52] at map at <console>:42\r\n",
       "transactionsJoinedRdd: org.apache.spark.rdd.RDD[(String, String, String, Int, String, Double)] = MapPartitionsRDD[57] at map at <console>:53\r\n"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// total spendings per customer\n",
    "val customerSpendingsRdd = transactionsRdd\n",
    "  .map(t => (t.custId, t.amount))\n",
    "  .reduceByKey(_ + _)\n",
    "\n",
    "// customer spendings classification\n",
    "val classifiedSpendingsRdd = customerSpendingsRdd.map { case (custId, totalSpend) =>\n",
    "  val spendingClass = if (totalSpend < 1000.0) \"Poor\"\n",
    "                      else if (totalSpend < 10000.0) \"Middle\"\n",
    "                      else \"Rich\"\n",
    "  (custId, CustomerSpending(totalSpend, spendingClass))\n",
    "}\n",
    "\n",
    "// inner join transactions with classified spendings\n",
    "val transactionsJoinedRdd = transactionsRdd\n",
    "  .map(t => (t.custId, t))\n",
    "  .join(classifiedSpendingsRdd)  // (custId, (Transaction, CustomerSpending))\n",
    "  .map { case (custId, (transaction, custSpending)) =>\n",
    "    (transaction.custId, transaction.transId, custSpending.spendingClass, transaction.year, transaction.expType, transaction.amount)\n",
    "  }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f39e7d-4252-437a-a601-a97d2d15a927",
   "metadata": {},
   "source": [
    "### Spending Analysis by Category and Year\n",
    "\n",
    "This job aims to aggregate customer transactions based on **spending class** (`spendingClass`) and **year** (`year`), computing key metrics for each combination.\n",
    "\n",
    "#### **Main Steps**:\n",
    "1. **Transaction Aggregation**  \n",
    "   - Transactions are grouped by (`spendingClass`, `year`).  \n",
    "   - The total spending amount (`totalAmount`) is computed.  \n",
    "   - The unique set of customers who made purchases (`custSet`) is collected.  \n",
    "\n",
    "2. **Metric Calculation**  \n",
    "   - The total number of customers (`customerCount`) for each (`spendingClass`, `year`).  \n",
    "   - The average spending per customer (`avgSpend = totalAmount / customerCount`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8a9f9d69-a2e7-4d80-b7f3-9d80363c9b2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "aggregatedByCategoryYear: org.apache.spark.rdd.RDD[((String, Int), (Double, scala.collection.immutable.Set[String]))] = ShuffledRDD[59] at reduceByKey at <console>:40\r\n",
       "metricsByCategoryYear: org.apache.spark.rdd.RDD[((String, Int), (Double, Int, Double))] = MapPartitionsRDD[60] at map at <console>:44\r\n"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// criteria: (spending class, year)\n",
    "\n",
    "// Mapping: ((spendingClass, year), (amount, Set(customerId)))\n",
    "val aggregatedByCategoryYear = transactionsJoinedRdd\n",
    "  .map { case (custId, transId, spendingClass, year, expType, amount) =>\n",
    "    ((spendingClass, year), (amount, Set(custId)))\n",
    "  }\n",
    "  .reduceByKey { case ((amount1, custSet1), (amount2, custSet2)) =>\n",
    "    (amount1 + amount2, custSet1 ++ custSet2)\n",
    "  }\n",
    "\n",
    "val metricsByCategoryYear = aggregatedByCategoryYear.map { case ((spendingClass, year), (totalAmount, custSet)) =>\n",
    "  val customerCount = custSet.size\n",
    "  val avgSpend = totalAmount / customerCount\n",
    "  ((spendingClass, year), (totalAmount, customerCount, avgSpend))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec04a74-80de-4827-805e-8a9090529b61",
   "metadata": {},
   "source": [
    "## Spending trends\n",
    "\n",
    "Previous calculated metrics can be useful, and I choose to use them in order to calculate spending trends, based on growth rate and retention rate.\n",
    "\n",
    "#### **Main Steps**:  \n",
    "1. **Grouping Data by Spending Class**:  \n",
    "   - We first map the dataset to `(spendingClass, (year, totalAmount, customerCount, avgSpend))` to enable time-based trend analysis.  \n",
    "   - Then, we use `groupByKey()` to aggregate all years for each `spendingClass`.  \n",
    "\n",
    "2. **Sorting by Year**:  \n",
    "   - Inside each spending class group, we sort data by `year` to track changes over time.  \n",
    "\n",
    "3. **Calculating Growth Rate and Retention Rate**:  \n",
    "   - We use a **sliding window of size 2** to compare consecutive years.  \n",
    "   - **Growth Rate** is computed as the percentage change in `avgSpend` from the previous year.  \n",
    "   - **Retention Rate** is calculated as the percentage of customers retained from the previous year.  \n",
    "   - If there is no previous year, the values default to `0.0%` growth and `100%` retention.  \n",
    "\n",
    "4. **Flattening the Output**:  \n",
    "   - We return a structured dataset `(spendingClass, year, totalAmount, customerCount, avgSpend, growthRate, retentionRate)`, which can be further analyzed or visualized.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d12a5689-f7b8-4a41-b5f9-130ef40dc238",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spendingTrends: org.apache.spark.rdd.RDD[((String, Int), (Double, Int, Double, Double, Double))] = MapPartitionsRDD[63] at flatMap at <console>:37\r\n"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val spendingTrends = metricsByCategoryYear\n",
    "  .map { case ((spendingClass, year), (totalAmount, customerCount, avgSpend)) =>\n",
    "    (spendingClass, (year, totalAmount, customerCount, avgSpend))\n",
    "  }\n",
    "  .groupByKey() \n",
    "  .flatMap { case (spendingClass, data) =>\n",
    "    val sortedData = data.toList.sortBy(_._1) \n",
    "\n",
    "    val trendData = sortedData.sliding(2).map {\n",
    "      case List((prevYear, prevTotal, prevCust, prevAvg), (year, totalAmount, customerCount, avgSpend)) =>\n",
    "        val growthRate = (avgSpend - prevAvg) / prevAvg * 100\n",
    "        val retentionRate = customerCount.toDouble / prevCust * 100\n",
    "        ((spendingClass, year), (totalAmount, customerCount, avgSpend, growthRate, retentionRate))\n",
    "      case List((year, totalAmount, customerCount, avgSpend)) =>\n",
    "        ((spendingClass, year), (totalAmount, customerCount, avgSpend, 0.0, 100.0))\n",
    "    }\n",
    "    \n",
    "    trendData\n",
    "  }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6ae1adc0-0951-4eeb-8782-73b6a606d43d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res0: Array[((String, Int), (Double, Int, Double, Double, Double))] = Array(((Poor,2010),(29576.680000000004,639,46.28588419405322,0.0,100.0)), ((Poor,2011),(83349.50000000004,1709,48.77091866588651,5.368881928267389,267.4491392801252)), ((Poor,2012),(134143.23999999996,2652,50.581915535444935,3.713272005321379,155.17846693973084)), ((Poor,2013),(201802.95000000007,3538,57.03870830977956,12.765022253477271,133.40874811463047)), ((Poor,2014),(234921.54000000027,4055,57.9337953144267,1.5692624029735929,114.6127755794234)), ((Poor,2015),(272362.26999999984,4639,58.71141840913987,1.34226161171167,114.4019728729963)), ((Poor,2016),(342843.3500000002,5263,65.14219076572302,10.953188546339124,113.45117482215994)), ((Poor,2017),(370305.6299999997,5553,66.6856888168557,2.369429141067291,105.5101...\r\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spendingTrends.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbdcaea-4067-46db-b697-72a3aab4eb39",
   "metadata": {},
   "source": [
    "# Job optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16c625f-42f6-4e4e-a07c-a2f617bc9d7e",
   "metadata": {},
   "source": [
    "## caching transactionsRdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "58e00cbe-1b40-4448-8aa5-d2258af5de74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transactionsRdd: org.apache.spark.rdd.RDD[Transaction] = MapPartitionsRDD[48] at map at <console>:35\r\n"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transactionsRdd = dataRdd.map { line =>\n",
    "    val fields = line.split(\",\")\n",
    "    val formatter = DateTimeFormatter.ofPattern(\"yyyy-MM-dd\")\n",
    "    Transaction(\n",
    "        custId   = fields(0),\n",
    "        startDate = fields(1),\n",
    "        endDate   = fields(2),\n",
    "        transId   = fields(3),\n",
    "        date      = LocalDate.parse(fields(4), formatter),\n",
    "        year      = fields(5).toInt,\n",
    "        month     = fields(6).toInt,\n",
    "        day       = fields(7).toInt,\n",
    "        expType   = fields(8),\n",
    "        amount    = fields(9).toDouble\n",
    "    )\n",
    "}.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a92482-95cc-460f-8524-b77b9901dff5",
   "metadata": {},
   "source": [
    "## Using broadcast for segmentation\n",
    "\n",
    "Since it's just a simple treshold, broadcast can drastically improve the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6baaa4f5-6bbd-438c-8f7f-1fcbde2b9d23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spendingThresholds: org.apache.spark.broadcast.Broadcast[scala.collection.immutable.Map[String,Double]] = Broadcast(10)\r\n",
       "classifiedSpendingsRdd: org.apache.spark.rdd.RDD[(String, String)] = MapPartitionsRDD[49] at map at <console>:38\r\n"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val spendingThresholds = sc.broadcast(Map(\n",
    "  \"Poor\" -> 1000.0,\n",
    "  \"Middle\" -> 10000.0\n",
    "))\n",
    "\n",
    "val classifiedSpendingsRdd = customerSpendingsRdd.map { case (custId, totalSpend) =>\n",
    "  val spendingClass = if (totalSpend < spendingThresholds.value(\"Poor\")) \"Poor\"\n",
    "                      else if (totalSpend < spendingThresholds.value(\"Middle\")) \"Middle\"\n",
    "                      else \"Rich\"\n",
    "  (custId, spendingClass)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d395984e-94c6-46a4-a783-aa49701fac8e",
   "metadata": {},
   "source": [
    "## Removing useless fields\n",
    "\n",
    "This optimization is pretty simple but effective, this rdd only has 4 fields, since other ones are not used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "922f94fb-76a8-4aa7-ae10-ecdfb0edd4c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transactionsJoinedRdd: org.apache.spark.rdd.RDD[((CustomerSpending, Int), (Double, Int))] = MapPartitionsRDD[68] at map at <console>:36\r\n"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val transactionsJoinedRdd = transactionsRdd\n",
    "  .map(t => (t.custId, (t.transId, t.year, t.expType, t.amount)))\n",
    "  .join(classifiedSpendingsRdd)\n",
    "  .map { case (custId, ((transId, year, expType, amount), spendingClass)) =>\n",
    "    ((spendingClass, year), (amount, 1))\n",
    "  }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afc3f44-afac-4128-97b3-a84d7c7629ce",
   "metadata": {},
   "source": [
    "## Caching rdd for reuse\n",
    "\n",
    "It has been preferred to cache only metricsByCategory year, in order to gain balance and avoid an excessive memory usage, that's why transactionsJoinedRdd hasn't been cached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2bae98d1-d94b-4a2a-8f00-1e3151bd9a50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "metricsByCategoryYear: org.apache.spark.rdd.RDD[((CustomerSpending, Int), (Double, Int, Double))] = MapPartitionsRDD[70] at map at <console>:36\r\n"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val metricsByCategoryYear = transactionsJoinedRdd\n",
    "  .reduceByKey { case ((amount1, count1), (amount2, count2)) =>\n",
    "    (amount1 + amount2, count1 + count2)\n",
    "  }\n",
    "  .map { case ((spendingClass, year), (totalAmount, customerCount)) =>\n",
    "    val avgSpend = totalAmount / customerCount\n",
    "    ((spendingClass, year), (totalAmount, customerCount, avgSpend))\n",
    "  }\n",
    "  .cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c429fe6-e211-4754-8dd1-c0669f626420",
   "metadata": {},
   "outputs": [],
   "source": [
    "## GroupByKey\n",
    "\n",
    "As it was stated in classes, even though GroupByKey is simple to understand, it can be bad performance wise, that's why in the optimization, reduceByKey was preferred. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3accf279-e9b1-44c9-abb5-a7222b189ecf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spendingTrends: org.apache.spark.rdd.RDD[((CustomerSpending, Int), (Double, Int, Double, Double, Double))] = MapPartitionsRDD[81] at flatMap at <console>:51\r\n"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val spendingTrends = metricsByCategoryYear\n",
    "  .map { case ((spendingClass, year), (totalAmount, customerCount, avgSpend)) =>\n",
    "    (spendingClass, List((year, totalAmount, customerCount, avgSpend)))\n",
    "  }\n",
    "  .reduceByKey(_ ++ _)\n",
    "  .mapValues { data =>\n",
    "    val sortedData = data.sortBy { case (year, _, _, _) => year }\n",
    "    \n",
    "    var prevAvgSpend = 0.0\n",
    "    var prevCustCount = 0\n",
    "    sortedData.map { case (year, totalAmount, customerCount, avgSpend) =>\n",
    "      val growthRate = if (prevAvgSpend > 0) ((avgSpend - prevAvgSpend) / prevAvgSpend) * 100 else 0.0\n",
    "      val retentionRate = if (prevCustCount > 0) (customerCount.toDouble / prevCustCount) * 100 else 100.0\n",
    "\n",
    "      prevAvgSpend = avgSpend\n",
    "      prevCustCount = customerCount\n",
    "      (year, totalAmount, customerCount, avgSpend, growthRate, retentionRate)\n",
    "    }\n",
    "  }\n",
    "  .flatMap { case (spendingClass, trends) =>\n",
    "    trends.map { case (year, totalAmount, customerCount, avgSpend, growthRate, retentionRate) =>\n",
    "      ((spendingClass, year), (totalAmount, customerCount, avgSpend, growthRate, retentionRate))\n",
    "    }\n",
    "  }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a4725c-2050-400c-9751-3b151cfc62c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
